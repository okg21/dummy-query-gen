{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import importlib\n",
    "\n",
    "import constants # Required for the following line if kernel is restarted\n",
    "importlib.reload(constants) # Else the old key value is retained\n",
    "from constants import openai_api\n",
    "\n",
    "client = OpenAI(api_key=openai_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DummyQueryGenerator:\n",
    "    def __init__(self, dataset_name=\"google_trends\"):\n",
    "        self.categories = pd.read_csv(f\"data/{dataset_name}.csv\")\n",
    "        self.categories['embedding'] = self.categories['embedding'].apply(lambda x: np.fromstring(x.strip('[]'), dtype=float, sep=' '))\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.similarities = None\n",
    "        \n",
    "        self.style_prompt = \"\"\n",
    "\n",
    "    def get_embedding(self, query: str):\n",
    "        inputs = self.tokenizer(query, return_tensors='pt', truncation=True, padding=True)\n",
    "        outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
    "\n",
    "    def analyze_query_style(self, query: str) -> Dict:\n",
    "        style_features = {\n",
    "            'length': len(query.split()),\n",
    "            'has_question_mark': '?' in query,\n",
    "            'starts_with_question_word': any(query.lower().startswith(w) for w in ['how', 'what', 'where', 'when', 'why', 'who']),\n",
    "            'capitalization': query[0].isupper() if query else False,\n",
    "            'lowercase_ratio': sum(1 for c in query if c.islower()) / len(query) if query else 0\n",
    "        }\n",
    "        return style_features\n",
    "\n",
    "    def identify_query_category(self, query: str) -> str:\n",
    "        query_embedding = self.get_embedding(query)\n",
    "\n",
    "        similarities = self.categories['embedding'].apply(lambda x: cosine_similarity(np.array(query_embedding).reshape(1, -1), x.reshape(1, -1))[0][0])\n",
    "        self.similarities = similarities\n",
    "        most_similar_category = self.categories.iloc[self.similarities.idxmax()]\n",
    "        return most_similar_category['category']        \n",
    "\n",
    "    def get_distant_categories(self, num_categories: int = 20) -> List[str]:\n",
    "        embeddings = np.array(self.categories['embedding'].tolist())\n",
    "        \n",
    "        from sklearn.cluster import KMeans\n",
    "        n_clusters = min(num_categories, len(self.categories))\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(embeddings)\n",
    "        \n",
    "        query_cluster_distances = kmeans.transform(embeddings[self.similarities.idxmax()].reshape(1, -1))[0]\n",
    "        \n",
    "        selected_categories = []\n",
    "        cluster_assignments = {i: [] for i in range(n_clusters)}\n",
    "        \n",
    "        for idx, cluster in enumerate(cluster_labels):\n",
    "            cluster_assignments[cluster].append(idx)\n",
    "        \n",
    "        samples_per_cluster = num_categories // n_clusters\n",
    "        remaining_samples = num_categories % n_clusters\n",
    "        \n",
    "        for cluster_idx in range(n_clusters):\n",
    "            cluster_indices = cluster_assignments[cluster_idx]\n",
    "            \n",
    "            n_samples = samples_per_cluster + (1 if cluster_idx < remaining_samples else 0)\n",
    "            \n",
    "            if cluster_indices:\n",
    "                sampled_indices = random.sample(cluster_indices, min(n_samples, len(cluster_indices)))\n",
    "                selected_categories.extend(self.categories.iloc[sampled_indices]['category'].tolist())\n",
    "        \n",
    "        random.shuffle(selected_categories)\n",
    "        \n",
    "        return selected_categories\n",
    "    \n",
    "    def visualize_category_distribution(self, original_query: str, selected_categories: List[str]):\n",
    "        selected_indices = self.categories[self.categories['category'].isin(selected_categories)].index\n",
    "        all_embeddings = np.array(self.categories['embedding'].tolist())\n",
    "        all_categories = self.categories['category'].tolist()\n",
    "        \n",
    "        query_embedding = np.array(self.get_embedding(original_query)).reshape(1, -1)\n",
    "        \n",
    "        combined_embeddings = np.vstack([all_embeddings, query_embedding])\n",
    "        \n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        embeddings_2d = tsne.fit_transform(combined_embeddings)\n",
    "        \n",
    "        categories_2d = embeddings_2d[:-1]\n",
    "        query_2d = embeddings_2d[-1]\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        plt.scatter(categories_2d[:, 0], categories_2d[:, 1], \n",
    "                alpha=0.1, color='gray', label='All Categories')\n",
    "        \n",
    "        plt.scatter(categories_2d[selected_indices, 0], categories_2d[selected_indices, 1], \n",
    "                color='blue', alpha=0.6, label='Selected Categories')\n",
    "        \n",
    "        for idx in selected_indices:\n",
    "            plt.annotate(all_categories[idx], \n",
    "                        (categories_2d[idx, 0], categories_2d[idx, 1]),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        fontsize=8, alpha=0.8)\n",
    "        \n",
    "        plt.scatter(query_2d[0], query_2d[1], \n",
    "                color='red', s=200, marker='*', label='Original Query')\n",
    "        plt.annotate('Original Query', \n",
    "                    (query_2d[0], query_2d[1]),\n",
    "                    xytext=(10, 10), textcoords='offset points',\n",
    "                    fontsize=10, color='red', fontweight='bold')\n",
    "        \n",
    "        plt.legend(fontsize=10)\n",
    "        plt.title('Distribution of Categories and Original Query in Embedding Space', \n",
    "                fontsize=12, pad=20)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def visualize_query_trajectories(self, original_query: str, dummy_queries: List[dict], follow_up_queries: List[dict]):\n",
    "        from sklearn.manifold import TSNE\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        all_embeddings = np.array(self.categories['embedding'].tolist())\n",
    "        all_categories = self.categories['category'].tolist()\n",
    "        \n",
    "        original_embedding = np.array(self.get_embedding(original_query)).reshape(1, -1)\n",
    "        dummy_embeddings = np.vstack([self.get_embedding(q['query']) for q in dummy_queries])\n",
    "        followup_embeddings = np.vstack([self.get_embedding(q['query']) for q in follow_up_queries])\n",
    "        \n",
    "        combined_embeddings = np.vstack([\n",
    "            all_embeddings,\n",
    "            original_embedding,\n",
    "            dummy_embeddings,\n",
    "            followup_embeddings\n",
    "        ])\n",
    "        \n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        embeddings_2d = tsne.fit_transform(combined_embeddings)\n",
    "        \n",
    "        categories_2d = embeddings_2d[:len(all_embeddings)]\n",
    "        original_2d = embeddings_2d[len(all_embeddings)]\n",
    "        dummy_2d = embeddings_2d[len(all_embeddings)+1:len(all_embeddings)+1+len(dummy_queries)]\n",
    "        followup_2d = embeddings_2d[-len(follow_up_queries):]\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        plt.scatter(categories_2d[:, 0], categories_2d[:, 1], \n",
    "                alpha=0.1, color='gray', label='Categories')\n",
    "        \n",
    "        plt.scatter(original_2d[0], original_2d[1], \n",
    "                color='red', s=200, marker='*', label='Original Query')\n",
    "        \n",
    "        for i in range(len(dummy_queries)):\n",
    "            plt.scatter(dummy_2d[i, 0], dummy_2d[i, 1], \n",
    "                    color='blue', alpha=0.6, label='Dummy Query' if i == 0 else \"\")\n",
    "            \n",
    "            plt.scatter(followup_2d[i, 0], followup_2d[i, 1], \n",
    "                    color='green', alpha=0.6, label='Follow-up Query' if i == 0 else \"\")\n",
    "            \n",
    "            plt.arrow(dummy_2d[i, 0], dummy_2d[i, 1],\n",
    "                    followup_2d[i, 0] - dummy_2d[i, 0],\n",
    "                    followup_2d[i, 1] - dummy_2d[i, 1],\n",
    "                    head_width=0.3, head_length=0.5, fc='k', ec='k', alpha=0.3)\n",
    "            \n",
    "            plt.annotate(f\"D{i+1}: {dummy_queries[i]['query'][:30]}...\",\n",
    "                        (dummy_2d[i, 0], dummy_2d[i, 1]),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        fontsize=8, alpha=0.8)\n",
    "            plt.annotate(f\"F{i+1}: {follow_up_queries[i]['query'][:30]}...\",\n",
    "                        (followup_2d[i, 0], followup_2d[i, 1]),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        fontsize=8, alpha=0.8)\n",
    "        \n",
    "        plt.title('Query Trajectory Visualization\\nShowing Original Query, Dummy Queries, and Follow-ups',\n",
    "                fontsize=12, pad=20)\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def generate_dummy_queries(self, input_query: str, num_queries: int = 20, show_plots: bool = True, use_new_prompt: bool = False):\n",
    "        query_style = self.analyze_query_style(input_query)\n",
    "        \n",
    "        input_category = self.identify_query_category(input_query)\n",
    "        \n",
    "        distant_categories = self.get_distant_categories(num_queries)  # Get enough categories for individual queries\n",
    "        if show_plots:\n",
    "            self.visualize_category_distribution(input_query, distant_categories)\n",
    "        \n",
    "        style_prompt = f\"\"\"\n",
    "        Generate a query that matches these style characteristics:\n",
    "        - Similar length (around {query_style['length']} words)\n",
    "        - {'Use' if query_style['has_question_mark'] else 'Avoid'} question marks\n",
    "        - {'Start with question words' if query_style['starts_with_question_word'] else 'Use declarative form'}\n",
    "        - {'Capitalize first letter' if query_style['capitalization'] else 'Use lowercase'}\n",
    "        \"\"\"\n",
    "    \n",
    "        all_queries = []\n",
    "        if use_new_prompt:\n",
    "            print('Input category:' + input_category)\n",
    "        print(distant_categories)\n",
    "    \n",
    "        for category in distant_categories:\n",
    "            system_message = f\"\"\"You are a query generation assistant. Generate a single query that:\n",
    "            1. Is specifically about {category}\n",
    "            2. Matches the original query's style\n",
    "            {style_prompt}\n",
    "            \"\"\"\n",
    "            \n",
    "            if use_new_prompt:\n",
    "                system_message += f\"\"\"\n",
    "                Here is an example query based on a category. But yours should not be exactly the same:\n",
    "                Category: {input_category}\n",
    "                Query: {input_query}\n",
    "                \"\"\"\n",
    "            \n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini-2024-07-18\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_message},\n",
    "                    {\"role\": \"user\", \"content\": \"\"\"Generate one search query.\n",
    "                    Format your response as JSON with the following structure:\n",
    "                    {\n",
    "                        \"queries\": [\n",
    "                            {\"query\": \"query text\", \"category\": \"category_name\"}\n",
    "                        ]\n",
    "                    }\"\"\"}\n",
    "                ],\n",
    "                response_format={ \"type\": \"json_object\" },\n",
    "                temperature=0.7,\n",
    "                seed=random.randint(0, 1000000)\n",
    "            )\n",
    "            \n",
    "            query_result = parse_dummy_queries(completion.choices[0].message.content)\n",
    "            all_queries.extend(query_result)\n",
    "            \n",
    "            if len(all_queries) >= num_queries:\n",
    "                break\n",
    "    \n",
    "        return all_queries[:num_queries]\n",
    "\n",
    "\n",
    "    def generate_consecutive_queries(self, input_query, dummy_queries, old_input_query=None, use_new_prompt: bool = False):\n",
    "        query_style = self.analyze_query_style(input_query)\n",
    "        \n",
    "        style_prompt = f\"\"\"\n",
    "        Generate a query that matches these style characteristics:\n",
    "        - Similar length (around {query_style['length']} words)\n",
    "        - {'Use' if query_style['has_question_mark'] else 'Avoid'} question marks\n",
    "        - {'Start with question words' if query_style['starts_with_question_word'] else 'Use declarative form'}\n",
    "        - {'Capitalize first letter' if query_style['capitalization'] else 'Use lowercase'}\n",
    "        \"\"\"\n",
    "        all_queries = []\n",
    "        for dummy_query in dummy_queries:\n",
    "            system_message = f\"\"\"You are a query generation assistant. Generate a single query that:\n",
    "            1. Will act as a follow-up to the query: {dummy_query['query']} \n",
    "            2. Matches the following style characteristics:\n",
    "            {style_prompt}\n",
    "            \"\"\"\n",
    "            \n",
    "            if use_new_prompt and old_input_query is not None:\n",
    "                system_message += f\"\"\"\n",
    "                Here is an example follow-up query based on a query. But yours should not be exactly the same:\n",
    "                Old Query: {old_input_query}\n",
    "                Follow-up query: {input_query}\n",
    "                \"\"\"\n",
    "            \n",
    "            follow_up_query = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini-2024-07-18\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_message},\n",
    "                    {\"role\": \"user\", \"content\": \"\"\"Generate one search query.\n",
    "                    Format your response as JSON with the following structure:\n",
    "                    {\n",
    "                        \"queries\": [\n",
    "                            {\"query\": \"query text\"}\n",
    "                        ]\n",
    "                    }\"\"\"}\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                seed=random.randint(0, 1000000)\n",
    "            )\n",
    "\n",
    "            query_result = parse_dummy_queries(follow_up_query.choices[0].message.content)\n",
    "            all_queries.extend(query_result)\n",
    "                \n",
    "        return all_queries\n",
    "\n",
    "import re\n",
    "def parse_dummy_queries(response):\n",
    "    cleaned_response = re.sub(r'```json|```', '', response)\n",
    "    cleaned_response = cleaned_response.strip()\n",
    "\n",
    "    return json.loads(cleaned_response)['queries']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "generator = DummyQueryGenerator()\n",
    "dummy_queries = generator.generate_dummy_queries(\"cancer treatment\")\n",
    "print('Initial dummy queries: ')\n",
    "for query in dummy_queries:\n",
    "    print(query)\n",
    "\n",
    "consecutive_queries = generator.generate_consecutive_queries(\"cancer symptoms\", dummy_queries)\n",
    "print('Consecutive queries: ')\n",
    "for query in consecutive_queries:\n",
    "    print(query)\n",
    "    \n",
    "generator.visualize_query_trajectories(\"cancer treatment\", dummy_queries, consecutive_queries)\n",
    "\n",
    "consecutive_queries_2 = generator.generate_consecutive_queries(\"chemotherapy side effects\", consecutive_queries)\n",
    "print('Consecutive queries 2: ')\n",
    "for query in consecutive_queries_2:\n",
    "    print(query)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def create_output_file(num_queries, original_queries, dataset_name, prompt_changed):\n",
    "    generator = DummyQueryGenerator(dataset_name=dataset_name)\n",
    "    \n",
    "    result = [[] for _ in range(num_queries)]\n",
    "    for oq in original_queries:\n",
    "        result[0].append(oq)\n",
    "    \n",
    "    dummy_queries = generator.generate_dummy_queries(original_queries[0], num_queries=num_queries - 1, show_plots=False, use_new_prompt=prompt_changed)\n",
    "    for i, q in enumerate(dummy_queries):\n",
    "        result[i + 1].append(q[\"query\"])\n",
    "    \n",
    "    for i in range(len(original_queries)-1):\n",
    "        if prompt_changed:\n",
    "            dummy_queries = generator.generate_consecutive_queries(original_queries[i+1], dummy_queries, original_queries[i], use_new_prompt=prompt_changed)\n",
    "        else:\n",
    "            dummy_queries = generator.generate_consecutive_queries(original_queries[i+1], dummy_queries)\n",
    "        for i, q in enumerate(dummy_queries):\n",
    "            result[i + 1].append(q[\"query\"])\n",
    "    \n",
    "    os.makedirs(os.path.dirname(\"outputs/\"), exist_ok=True)\n",
    "    prompt_str = \"_p2\" if prompt_changed else \"\"\n",
    "    output_file = f\"outputs/{dataset_name}_r{num_queries}_c{len(original_queries)}{prompt_str}.csv\"\n",
    "    with open(output_file, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "    \n",
    "        writer.writerow([f\"Query {i + 1}\" for i in range(len(original_queries))])\n",
    "    \n",
    "        for row in result:\n",
    "            writer.writerow(row)\n",
    "    \n",
    "    print(f\"{output_file} generated.\")\n",
    "\n",
    "def create_all_output_files(original_queries, prompt_changed):\n",
    "    datasets = [\"google_trends\", \"wiki_categories_50k\", \"wiki_categories_100k\"]\n",
    "    num_queries = [10, 30, 50]\n",
    "    num_consec = [1, 3, 5]\n",
    "    for ds in datasets:\n",
    "        for nq in num_queries:\n",
    "            for nc in num_consec:\n",
    "                try:\n",
    "                    create_output_file(nq, original_queries[:nc], ds, prompt_changed)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error creating output file for dataset: {ds}, num_queries: {nq}, num_consec: {nc}. Error: {e}\")\n",
    "\n",
    "\n",
    "dataset_name = \"wiki_categories_50k\" #\"google_trends\"\n",
    "prompt_changed = False\n",
    "num_queries = 10\n",
    "original_queries = [\n",
    "    \"best pizza\",\n",
    "    \"margherita pizza\",\n",
    "    \"which cheese for margherita pizza\",\n",
    "    \"where to find swiss cheese\",\n",
    "    \"why does swiss cheese has holes\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_output_file(num_queries, original_queries, dataset_name, prompt_changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_all_output_files(original_queries, prompt_changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
