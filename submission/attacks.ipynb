{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c193ccd6-ff43-4cd1-8566-9bbbf1330b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import importlib\n",
    "from openai import OpenAI\n",
    "\n",
    "import constants # Required for the following line if kernel is restarted\n",
    "importlib.reload(constants) # Else the old key value is retained\n",
    "from constants import openai_api\n",
    "\n",
    "client = OpenAI(api_key=openai_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2284d373-8304-4662-bd5b-35b003584b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import json\n",
    "import random\n",
    "\n",
    "def attack_with_random(csv_filename):\n",
    "    with open(csv_filename, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        rows = list(reader)\n",
    "\n",
    "    headers = rows[0]\n",
    "    data = rows[1:]\n",
    "    \n",
    "    original_row_0 = data[0]\n",
    "    random.shuffle(data)\n",
    "    new_index_of_row_0 = data.index(original_row_0)\n",
    "    shuffled_rows = [headers] + data\n",
    "    \n",
    "    indices = list(range(len(data)))\n",
    "    random.shuffle(indices)\n",
    "    top_10_indices = indices[:10]\n",
    "    \n",
    "    result = {str(rank + 1): idx for rank, idx in enumerate(top_10_indices)}\n",
    "    result = json.dumps(result)\n",
    "    return result, new_index_of_row_0\n",
    "\n",
    "def attack_with_gpt(csv_filename, whitebox_prompt=False):\n",
    "    if whitebox_prompt:\n",
    "        def extract_whitebox_categories():\n",
    "            categories = set()\n",
    "            with open(\"data/google_trends.csv\", mode='r') as file:\n",
    "                reader = csv.reader(file)\n",
    "                next(reader)\n",
    "                for row in reader:\n",
    "                    categories.add(row[0])\n",
    "            return sorted(categories)\n",
    "    \n",
    "        whitebox_categories = extract_whitebox_categories()\n",
    "        whitebox_categories = \", \".join(whitebox_categories)\n",
    "\n",
    "    with open(csv_filename, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        rows = list(reader)\n",
    "\n",
    "    headers = rows[0]\n",
    "    data = rows[1:]\n",
    "\n",
    "    original_row_0 = data[0]\n",
    "    random.shuffle(data)\n",
    "    new_index_of_row_0 = data.index(original_row_0)\n",
    "    shuffled_rows = [headers] + data\n",
    "\n",
    "    formatted_data = \"\\n\".join([\", \".join(row) for row in shuffled_rows])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a query analysis assistant. Below is a CSV file containing rows of sequential queries.\n",
    "    Each row corresponds to a different topic, and each column represents a sequential query flow.\n",
    "    Only one of the rows is a genuine query flow made by a human user; the rest are AI-generated\n",
    "    to thwart any adversary from figuring out the actual search intention. Your task is to \n",
    "    analyze the queries in each row and identify which one is the real, human-generated query. \n",
    "    Rank your predictions and return the top 10. Return them in a JSON format where the key is \n",
    "    the prediction rank and the value is the row index of the original query.\n",
    "\n",
    "    CSV Data:\n",
    "    {formatted_data}\n",
    "    \"\"\"\n",
    "\n",
    "    if whitebox_prompt:\n",
    "        whitebox_info = f\"\"\"\n",
    "        The AI-generated prompts are generated by selecting a category among the following categories.\n",
    "        If a query is similar to these categories, it has a higher chance of being AI-generated. \n",
    "        Categories: {whitebox_categories}\n",
    "        \"\"\"\n",
    "        prompt = \"\\n\" + prompt + whitebox_info\n",
    "\n",
    "    #print(prompt)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini-2024-07-18\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a query analysis assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        temperature=0.7,\n",
    "        seed=random.randint(0, 1000000)\n",
    "    )\n",
    "\n",
    "    result = response.choices[0].message.content\n",
    "    return result, new_index_of_row_0\n",
    "\n",
    "def evaluate_results(attack_result, new_index_of_row_0):\n",
    "    results = json.loads(attack_result)\n",
    "    for rank, row_index in results.items():\n",
    "        if row_index == new_index_of_row_0:\n",
    "            print(f\"Original query was detected as rank {rank}\")\n",
    "            return int(rank)\n",
    "    return -1\n",
    "\n",
    "def attack_and_eval(n_attacks, csv_filename, attack_type=\"gpt\", whitebox_prompt=False):\n",
    "    success_rank_sum = 0\n",
    "    success_count_sum = 0\n",
    "    for i in range(n_attacks):\n",
    "        try:\n",
    "            if attack_type == \"gpt\":\n",
    "                attack_result, new_index_of_row_0 = attack_with_gpt(csv_filename, whitebox_prompt=whitebox_prompt)\n",
    "            else:\n",
    "                attack_result, new_index_of_row_0 = attack_with_random(csv_filename)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        rank = evaluate_results(attack_result, new_index_of_row_0)\n",
    "        if rank > 0:\n",
    "            success_rank_sum += rank\n",
    "            success_count_sum += 1\n",
    "    try:\n",
    "        success_rank_sum = success_rank_sum / success_count_sum\n",
    "    except ZeroDivisionError as e:\n",
    "        success_rank_sum = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        success_count_sum = success_count_sum / n_attacks\n",
    "    except ZeroDivisionError as e:\n",
    "        success_count_sum = \"N/A\"\n",
    "        \n",
    "    print(f\"Average success rank: {success_rank_sum}\")\n",
    "    print(f\"Average success count: {success_count_sum}\")\n",
    "\n",
    "    return success_rank_sum, success_count_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3062cdec-6512-4085-b916-66213f42a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_and_eval(n_attacks=10, csv_filename=\"outputs/wiki_categories_50k_r10_c1_p2.csv\", whitebox_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deb3a1c-c672-4e2e-93fc-d06b8fcd6b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "results = {}\n",
    "#all_csv_files = glob.glob(\"outputs/google_trends*_p2.csv\")\n",
    "all_csv_files = [file for file in glob.glob(\"outputs/*.csv\") if \"_p2\" not in file]\n",
    "for fn in all_csv_files:\n",
    "    success_rank_sum, success_count_sum = attack_and_eval(n_attacks=100, csv_filename=fn, attack_type=\"gpt\", whitebox_prompt=True)\n",
    "    results[fn] = (success_rank_sum, success_count_sum)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0f0779-5773-4fc0-ae02-96786230892c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
