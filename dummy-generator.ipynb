{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import importlib\n",
    "\n",
    "import constants # Required for the following line if kernel is restarted\n",
    "importlib.reload(constants) # Else the old key value is retained\n",
    "from constants import openai_api\n",
    "\n",
    "client = OpenAI(api_key=openai_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebk/anaconda3/envs/irs_proj/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DummyQueryGenerator:\n",
    "    def __init__(self, dataset_name=\"google_trends\"):\n",
    "        self.categories = pd.read_csv(f\"data/{dataset_name}.csv\")\n",
    "        self.categories['embedding'] = self.categories['embedding'].apply(lambda x: np.fromstring(x.strip('[]'), dtype=float, sep=' '))\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.similarities = None\n",
    "        \n",
    "        self.style_prompt = \"\"\n",
    "\n",
    "    def get_embedding(self, query: str):\n",
    "        inputs = self.tokenizer(query, return_tensors='pt', truncation=True, padding=True)\n",
    "        outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
    "\n",
    "\n",
    "    # TODO: improve with more style features\n",
    "    def analyze_query_style(self, query: str) -> Dict:\n",
    "        style_features = {\n",
    "            'length': len(query.split()),\n",
    "            'has_question_mark': '?' in query,\n",
    "            'starts_with_question_word': any(query.lower().startswith(w) for w in ['how', 'what', 'where', 'when', 'why', 'who']),\n",
    "            'capitalization': query[0].isupper() if query else False,\n",
    "            'lowercase_ratio': sum(1 for c in query if c.islower()) / len(query) if query else 0\n",
    "        }\n",
    "        return style_features\n",
    "\n",
    "    # TODO: Identify within the embedding space\n",
    "    def identify_query_category(self, query: str) -> str:\n",
    "        # Get query embedding\n",
    "        query_embedding = self.get_embedding(query)\n",
    "        # Get most similar category with cosine similarity\n",
    "        similarities = self.categories['embedding'].apply(lambda x: cosine_similarity(np.array(query_embedding).reshape(1, -1), x.reshape(1, -1))[0][0])\n",
    "        self.similarities = similarities\n",
    "        most_similar_category = self.categories.iloc[self.similarities.idxmax()]\n",
    "        return most_similar_category['category']\n",
    "        \n",
    "        \n",
    "\n",
    "    # TODO: Discuss, and implement/change as needed\n",
    "    def get_distant_categories(self, num_categories: int = 20) -> List[str]:\n",
    "        # Get all embeddings as numpy array\n",
    "        embeddings = np.array(self.categories['embedding'].tolist())\n",
    "        \n",
    "        # 1. First divide the embedding space into regions using K-means\n",
    "        from sklearn.cluster import KMeans\n",
    "        n_clusters = min(num_categories, len(self.categories))\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(embeddings)\n",
    "        \n",
    "        # 2. Get cluster centers distances to query\n",
    "        query_cluster_distances = kmeans.transform(embeddings[self.similarities.idxmax()].reshape(1, -1))[0]\n",
    "        \n",
    "        # 3. Sample categories from each cluster\n",
    "        selected_categories = []\n",
    "        cluster_assignments = {i: [] for i in range(n_clusters)}\n",
    "        \n",
    "        # Group categories by cluster\n",
    "        for idx, cluster in enumerate(cluster_labels):\n",
    "            cluster_assignments[cluster].append(idx)\n",
    "        \n",
    "        # Calculate samples per cluster to ensure uniform distribution\n",
    "        samples_per_cluster = num_categories // n_clusters\n",
    "        remaining_samples = num_categories % n_clusters\n",
    "        \n",
    "        # Sample from each cluster\n",
    "        for cluster_idx in range(n_clusters):\n",
    "            cluster_indices = cluster_assignments[cluster_idx]\n",
    "            \n",
    "            # Calculate number of samples for this cluster\n",
    "            n_samples = samples_per_cluster + (1 if cluster_idx < remaining_samples else 0)\n",
    "            \n",
    "            if cluster_indices:  # If cluster is not empty\n",
    "                # Random sampling within cluster\n",
    "                sampled_indices = random.sample(cluster_indices, min(n_samples, len(cluster_indices)))\n",
    "                selected_categories.extend(self.categories.iloc[sampled_indices]['category'].tolist())\n",
    "        \n",
    "        # Shuffle the final selection to avoid any patterns\n",
    "        random.shuffle(selected_categories)\n",
    "        \n",
    "        return selected_categories\n",
    "    \n",
    "    def visualize_category_distribution(self, original_query: str, selected_categories: List[str]):\n",
    "        # Get embeddings and categories\n",
    "        selected_indices = self.categories[self.categories['category'].isin(selected_categories)].index\n",
    "        all_embeddings = np.array(self.categories['embedding'].tolist())\n",
    "        all_categories = self.categories['category'].tolist()\n",
    "        \n",
    "        # Get embedding for original query\n",
    "        query_embedding = np.array(self.get_embedding(original_query)).reshape(1, -1)\n",
    "        \n",
    "        # Combine embeddings for TSNE\n",
    "        combined_embeddings = np.vstack([all_embeddings, query_embedding])\n",
    "        \n",
    "        # Reduce dimensionality\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        embeddings_2d = tsne.fit_transform(combined_embeddings)\n",
    "        \n",
    "        # Separate categories and query coordinates\n",
    "        categories_2d = embeddings_2d[:-1]\n",
    "        query_2d = embeddings_2d[-1]\n",
    "        \n",
    "        # Create plot\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Plot all categories\n",
    "        plt.scatter(categories_2d[:, 0], categories_2d[:, 1], \n",
    "                alpha=0.1, color='gray', label='All Categories')\n",
    "        \n",
    "        # Plot selected categories with labels\n",
    "        plt.scatter(categories_2d[selected_indices, 0], categories_2d[selected_indices, 1], \n",
    "                color='blue', alpha=0.6, label='Selected Categories')\n",
    "        \n",
    "        # Add labels for selected categories\n",
    "        for idx in selected_indices:\n",
    "            plt.annotate(all_categories[idx], \n",
    "                        (categories_2d[idx, 0], categories_2d[idx, 1]),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        fontsize=8, alpha=0.8)\n",
    "        \n",
    "        # Plot and label original query\n",
    "        plt.scatter(query_2d[0], query_2d[1], \n",
    "                color='red', s=200, marker='*', label='Original Query')\n",
    "        plt.annotate('Original Query', \n",
    "                    (query_2d[0], query_2d[1]),\n",
    "                    xytext=(10, 10), textcoords='offset points',\n",
    "                    fontsize=10, color='red', fontweight='bold')\n",
    "        \n",
    "        # Styling\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.title('Distribution of Categories and Original Query in Embedding Space', \n",
    "                fontsize=12, pad=20)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Adjust layout to prevent label overlap\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def visualize_query_trajectories(self, original_query: str, dummy_queries: List[dict], follow_up_queries: List[dict]):\n",
    "        \"\"\"\n",
    "        Visualize the distribution of categories, original query, dummy queries and their follow-ups\n",
    "        \"\"\"\n",
    "        from sklearn.manifold import TSNE\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        # Get all category embeddings\n",
    "        all_embeddings = np.array(self.categories['embedding'].tolist())\n",
    "        all_categories = self.categories['category'].tolist()\n",
    "        \n",
    "        # Get embeddings for original, dummy and follow-up queries\n",
    "        original_embedding = np.array(self.get_embedding(original_query)).reshape(1, -1)\n",
    "        dummy_embeddings = np.vstack([self.get_embedding(q['query']) for q in dummy_queries])\n",
    "        followup_embeddings = np.vstack([self.get_embedding(q['query']) for q in follow_up_queries])\n",
    "        \n",
    "        # Combine all embeddings for TSNE\n",
    "        combined_embeddings = np.vstack([\n",
    "            all_embeddings,\n",
    "            original_embedding,\n",
    "            dummy_embeddings,\n",
    "            followup_embeddings\n",
    "        ])\n",
    "        \n",
    "        # Reduce dimensionality\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        embeddings_2d = tsne.fit_transform(combined_embeddings)\n",
    "        \n",
    "        # Separate different types of points\n",
    "        categories_2d = embeddings_2d[:len(all_embeddings)]\n",
    "        original_2d = embeddings_2d[len(all_embeddings)]\n",
    "        dummy_2d = embeddings_2d[len(all_embeddings)+1:len(all_embeddings)+1+len(dummy_queries)]\n",
    "        followup_2d = embeddings_2d[-len(follow_up_queries):]\n",
    "        \n",
    "        # Create plot\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Plot categories in background\n",
    "        plt.scatter(categories_2d[:, 0], categories_2d[:, 1], \n",
    "                alpha=0.1, color='gray', label='Categories')\n",
    "        \n",
    "        # Plot original query\n",
    "        plt.scatter(original_2d[0], original_2d[1], \n",
    "                color='red', s=200, marker='*', label='Original Query')\n",
    "        \n",
    "        # Plot dummy and follow-up queries with arrows\n",
    "        for i in range(len(dummy_queries)):\n",
    "            # Plot dummy query point\n",
    "            plt.scatter(dummy_2d[i, 0], dummy_2d[i, 1], \n",
    "                    color='blue', alpha=0.6, label='Dummy Query' if i == 0 else \"\")\n",
    "            \n",
    "            # Plot follow-up query point\n",
    "            plt.scatter(followup_2d[i, 0], followup_2d[i, 1], \n",
    "                    color='green', alpha=0.6, label='Follow-up Query' if i == 0 else \"\")\n",
    "            \n",
    "            # Draw arrow from dummy to follow-up\n",
    "            plt.arrow(dummy_2d[i, 0], dummy_2d[i, 1],\n",
    "                    followup_2d[i, 0] - dummy_2d[i, 0],\n",
    "                    followup_2d[i, 1] - dummy_2d[i, 1],\n",
    "                    head_width=0.3, head_length=0.5, fc='k', ec='k', alpha=0.3)\n",
    "            \n",
    "            # Add labels\n",
    "            plt.annotate(f\"D{i+1}: {dummy_queries[i]['query'][:30]}...\",\n",
    "                        (dummy_2d[i, 0], dummy_2d[i, 1]),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        fontsize=8, alpha=0.8)\n",
    "            plt.annotate(f\"F{i+1}: {follow_up_queries[i]['query'][:30]}...\",\n",
    "                        (followup_2d[i, 0], followup_2d[i, 1]),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        fontsize=8, alpha=0.8)\n",
    "        \n",
    "        plt.title('Query Trajectory Visualization\\nShowing Original Query, Dummy Queries, and Follow-ups',\n",
    "                fontsize=12, pad=20)\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def generate_dummy_queries(self, input_query: str, num_queries: int = 20, show_plots: bool = True):\n",
    "        # Analyze input query style\n",
    "        query_style = self.analyze_query_style(input_query)\n",
    "        \n",
    "        # Identify input category\n",
    "        input_category = self.identify_query_category(input_query)\n",
    "        \n",
    "        # Get distant categories\n",
    "        distant_categories = self.get_distant_categories(num_queries)  # Get enough categories for individual queries\n",
    "        if show_plots:\n",
    "            self.visualize_category_distribution(input_query, distant_categories)\n",
    "        \n",
    "        # Prepare style prompt\n",
    "        style_prompt = f\"\"\"\n",
    "        Generate a query that matches these style characteristics:\n",
    "        - Similar length (around {query_style['length']} words)\n",
    "        - {'Use' if query_style['has_question_mark'] else 'Avoid'} question marks\n",
    "        - {'Start with question words' if query_style['starts_with_question_word'] else 'Use declarative form'}\n",
    "        - {'Capitalize first letter' if query_style['capitalization'] else 'Use lowercase'}\n",
    "        \"\"\"\n",
    "\n",
    "        all_queries = []\n",
    "        print(distant_categories)\n",
    "        # Generate one query per category\n",
    "        for category in distant_categories:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini-2024-07-18\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": f\"\"\"You are a query generation assistant. Generate a single query that:\n",
    "                    1. Is completely unrelated to {input_category}\n",
    "                    2. Is specifically about {category}\n",
    "                    3. Matches the original query's style\n",
    "                    {style_prompt}\n",
    "                    Always respond with valid JSON containing a single query.\"\"\"},\n",
    "                    {\"role\": \"user\", \"content\": \"\"\"Generate one search query.\n",
    "                    Format your response as JSON with the following structure:\n",
    "                    {\n",
    "                        \"queries\": [\n",
    "                            {\"query\": \"query text\", \"category\": \"category_name\"}\n",
    "                        ]\n",
    "                    }\"\"\"}\n",
    "                ],\n",
    "                response_format={ \"type\": \"json_object\" },\n",
    "                temperature=0.7,\n",
    "                seed=random.randint(0, 1000000)\n",
    "            )\n",
    "            \n",
    "            # Parse and add the generated query\n",
    "            query_result = parse_dummy_queries(completion.choices[0].message.content)\n",
    "            all_queries.extend(query_result)\n",
    "            \n",
    "            # Break if we have enough queries\n",
    "            if len(all_queries) >= num_queries:\n",
    "                break\n",
    "\n",
    "        return all_queries[:num_queries]\n",
    "    \n",
    "    \n",
    "    # Consecutive query handling\n",
    "\n",
    "    def generate_consecutive_queries(self,input_query, dummy_queries):        \n",
    "        # Analyze input query style\n",
    "        query_style = self.analyze_query_style(input_query)\n",
    "        \n",
    "        # Prepare style prompt\n",
    "        style_prompt = f\"\"\"\n",
    "        Generate a query that matches these style characteristics:\n",
    "        - Similar length (around {query_style['length']} words)\n",
    "        - {'Use' if query_style['has_question_mark'] else 'Avoid'} question marks\n",
    "        - {'Start with question words' if query_style['starts_with_question_word'] else 'Use declarative form'}\n",
    "        - {'Capitalize first letter' if query_style['capitalization'] else 'Use lowercase'}\n",
    "        \"\"\"\n",
    "        all_queries = []\n",
    "        for dummy_query in dummy_queries:\n",
    "            # Generate a query that will act as a follow up to the dummy query\n",
    "            follow_up_query = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini-2024-07-18\",\n",
    "                messages=[{\"role\": \"system\", \"content\": f\"\"\"You are a query generation assistant. Generate a single query that:\n",
    "                        1. Will act as a follow up to the query: {dummy_query['query']} \n",
    "                        2. Matches the following style characteristics:\n",
    "                        {style_prompt}\n",
    "                        Always respond with valid JSON containing a single query.\"\"\"},\n",
    "                        {\"role\": \"user\", \"content\": \"\"\"Generate one search query.\n",
    "                        Format your response as JSON with the following structure:\n",
    "                        {\n",
    "                            \"queries\": [\n",
    "                                {\"query\": \"query text\"}\n",
    "                            ]\n",
    "                        }\"\"\"}],\n",
    "                # TODO: No need for json object\n",
    "                temperature=0.7,\n",
    "                seed=random.randint(0, 1000000)\n",
    "            )\n",
    "            query_result = parse_dummy_queries(follow_up_query.choices[0].message.content)\n",
    "\n",
    "            all_queries.extend(query_result)\n",
    "                \n",
    "        return all_queries\n",
    "    \n",
    "def parse_dummy_queries(response):\n",
    "    return json.loads(response)['queries']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "generator = DummyQueryGenerator()\n",
    "dummy_queries = generator.generate_dummy_queries(\"cancer treatment\")\n",
    "print('Initial dummy queries: ')\n",
    "for query in dummy_queries:\n",
    "    print(query)\n",
    "\n",
    "consecutive_queries = generator.generate_consecutive_queries(\"cancer symptoms\", dummy_queries)\n",
    "print('Consecutive queries: ')\n",
    "for query in consecutive_queries:\n",
    "    print(query)\n",
    "    \n",
    "generator.visualize_query_trajectories(\"cancer treatment\", dummy_queries, consecutive_queries)\n",
    "\n",
    "consecutive_queries_2 = generator.generate_consecutive_queries(\"chemotherapy side effects\", consecutive_queries)\n",
    "print('Consecutive queries 2: ')\n",
    "for query in consecutive_queries_2:\n",
    "    print(query)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def create_output_file(num_queries, original_queries, dataset_name):\n",
    "    generator = DummyQueryGenerator()\n",
    "    \n",
    "    result = [[] for _ in range(num_queries)]\n",
    "    for oq in original_queries:\n",
    "        result[0].append(oq)\n",
    "    \n",
    "    dummy_queries = generator.generate_dummy_queries(original_queries[0], num_queries=num_queries - 1, show_plots=False)\n",
    "    for i, q in enumerate(dummy_queries):\n",
    "        result[i + 1].append(q[\"query\"])\n",
    "    \n",
    "    for orig_query in original_queries[1:]:\n",
    "        dummy_queries = generator.generate_consecutive_queries(orig_query, dummy_queries)\n",
    "        for i, q in enumerate(dummy_queries):\n",
    "            result[i + 1].append(q[\"query\"])\n",
    "    \n",
    "    os.makedirs(os.path.dirname(\"outputs/\"), exist_ok=True)\n",
    "    output_file = f\"outputs/{dataset_name}_r{num_queries}_c{len(original_queries)}.csv\"\n",
    "    with open(output_file, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "    \n",
    "        writer.writerow([f\"Query {i + 1}\" for i in range(len(original_queries))])\n",
    "    \n",
    "        for row in result:\n",
    "            writer.writerow(row)\n",
    "    \n",
    "    print(f\"{output_file} generated.\")\n",
    "\n",
    "def create_all_output_files(original_queries):\n",
    "    datasets = [\"google_trends\", \"wiki_categories_50k\"]\n",
    "    num_queries = [10, 30, 50]\n",
    "    num_consec = [1, 3, 5]\n",
    "    for ds in datasets:\n",
    "        for nq in num_queries:\n",
    "            for nc in num_consec:\n",
    "                create_output_file(nq, original_queries[:nc], ds)\n",
    "\n",
    "dataset_name = \"google_trends\" #\"google_trends\"\n",
    "num_queries = 50\n",
    "original_queries = [\n",
    "    \"best pizza\",\n",
    "    \"margherita pizza\",\n",
    "    \"which cheese for margherita pizza\",\n",
    "    \"where to find swiss cheese\",\n",
    "    \"why does swiss cheese has holes\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Top Ten', 'News (Trending)', 'Olympians', 'Sport', 'When Is...', 'People of Sport', 'Films', 'Aussies', 'Fashion questions', 'Tech devices', 'Music Artists', 'Australian Public Figures', 'Fastest Rising', 'Global Football Players', 'Famous men', 'Beauty', 'Popular Sports-Related Queries', 'Who is...?', 'Cities', 'Female Celebrities', 'News Items of 2015', 'News Moments', 'Music Bands', 'Premier League Football', 'UK Number 1 Singles', 'Words', 'Global Public Figures', 'Travel Destinations', 'Song Lyrics', 'Loss (Trending)', 'Sportspeople', \"Top 'Tickets' Searches\", 'Losses', 'Overall searches', 'Digital/Internet Lingo', 'Fitness', 'Men', 'How to?', 'Kiwis', 'Sporting events', 'Coronavirus', 'New Zealand News Events', 'When is...?', 'What is', 'Top trending', 'How To...?', 'Sporting Events (Trending)', '\"How To\" Searches', 'Top Celebrities Searches']\n",
      "outputs/google_trends_r50_c5.csv generated.\n"
     ]
    }
   ],
   "source": [
    "create_output_file(num_queries, original_queries, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Digital/Internet Lingo', 'Aussies', 'What Is...', 'Fitness', 'Sporting Events (Trending)', 'Music Artists', 'When is', 'Overall', 'News events']\n",
      "outputs/wiki_categories_50k_r10_c1.csv generated.\n",
      "['How To', 'Overall searches', 'When Is...', 'News Searches', 'Loss', 'Digital/Internet Lingo', 'TV Shows', 'News (Trending)', 'Sports Events']\n",
      "outputs/wiki_categories_50k_r10_c3.csv generated.\n",
      "[\"Top 'John' Searches\", 'Overall Searches (Trending)', 'Men', 'News moments', 'Top questions', 'How to...?', 'What Is…?', 'News events', 'Aussies']\n",
      "outputs/wiki_categories_50k_r10_c5.csv generated.\n",
      "[\"Top 'John' Searches\", 'Aussie Public Figures', 'Where is?', 'News Items of 2013', 'TV Stars', 'When is', 'Top trending searches', 'Global people', 'UK Number 1 Singles', 'Kiwis (Trending)', 'Sports', 'Top Olympians', 'News events', 'Coronavirus', 'Lyrics', 'Sports events', 'Songs', 'Beauty', 'People of Sport', 'Losses', 'Overall Searches', 'Trending Beers', 'Digital / Internet Lingo', 'Top Ten', 'When Is...', 'Top Crises', 'New Zealanders', 'Women', 'News moments']\n",
      "outputs/wiki_categories_50k_r30_c1.csv generated.\n",
      "['Musicians', 'News Items of 2015', 'UK Number 1 Singles', 'Losses', 'News Searches', 'Top Ten', 'Popular Sports-Related Queries', 'Coronavirus', 'Kiwis', 'Fastest Rising', 'Cities', 'Song Lyrics', 'Popular Movies', 'Events', 'Diet', 'Olympians', 'Sports', 'When is', 'Who...?', 'Where is?', 'Global Football Players', '\"How To\" Searches', 'Male Celebrities', 'Men', 'Words', 'Famous men', 'Digital/Internet Lingo', 'Overall searches', 'Top Trending']\n",
      "outputs/wiki_categories_50k_r30_c3.csv generated.\n",
      "['Female Celebrities', 'Losses', 'Men', 'Sporting Events', 'Fitness', 'Top Trending', 'How to', \"Top 'Who Is' Searches\", 'Fastest Rising Searches ', 'How To...?', 'Politicians', 'Searches', 'Films', 'Digital/Internet Lingo', 'Top Olympians', '2013 Events', 'Top Ten', 'Music Artists', 'News Events', 'Beauty', 'Coronavirus', 'What is', 'UK Number 1 Singles', 'Sports Teams', 'Overall Searches', 'Lyrics', 'Sporting Events (Trending)', 'Kiwis', 'DIY']\n",
      "outputs/wiki_categories_50k_r30_c5.csv generated.\n",
      "['Overall', 'Fitness', 'Digital/Internet Lingo', 'Top Ten', 'Top questions', 'News items of 2014', 'Films', 'Politicians', 'Football Stars', 'Global Figures', 'Travel Destinations', 'New Zealanders', 'Premier League Football', 'What is?', 'Loss (Trending)', '\"How To\" Searches', 'Global people', 'Sports events', 'News Events', 'Beauty', 'Kiwis (Trending)', 'Losses', 'News moments', 'Sportspeople', 'UK Number 1 Singles', 'Olympians', 'How to', 'Words', 'What is...', 'Aussie Public Figures', 'Musicians', 'Who Is...', 'Popular Sports-Related Queries', 'Male Celebrities', 'Top Lyric Searches', 'People of Sport', 'Recipes', 'Most Popular', 'Lyrics', 'Coronavirus', 'Why is...?', 'Gadgets', 'Top trending', 'Men', 'When...?', 'Sport', 'Sporting Events (Trending)', 'Famous men', \"Top 'Who Is' Searches\"]\n",
      "outputs/wiki_categories_50k_r50_c1.csv generated.\n",
      "['Men', \"Top 'John' Searches\", 'Words', 'Global People', 'Fashion Labels', 'Coronavirus', 'Australian Celebrities', 'What is', 'Popular Sports-Related Queries', 'TV Shows, Trending', 'Travel Destinations', 'Digital / Internet Lingo', 'Music Artists', 'News Moments', 'New Zealand News Events', 'People of Sport', 'When Is...', 'Top Ten', 'Politicians', 'How to...?', 'News Items of 2013', 'Famous women', 'Top Celebrities Searches', 'Song Lyrics', 'Sports Personalities', 'Trending Beers', 'Who Is...', 'Fastest Rising', 'Why is...?', 'Global People (Trending)', 'Beauty', 'Popular Movies', 'Delivery', 'UK Number 1 Singles', 'Loss (Trending)', 'Diet', 'Olympians', 'Global Figures', 'Sporting Events (Trending)', 'How To', 'Australian Public Figures', 'Sport', 'Sporting events', 'Losses', 'Premier League Football', 'New Zealanders', 'Overall searches', '\"How To\" Searches', 'Football Stars']\n",
      "outputs/wiki_categories_50k_r50_c3.csv generated.\n",
      "['Digital/Internet Lingo', 'Fitness', 'Fashion questions', 'Words', 'Aussies (Trending)', 'Why is…?', 'Loss', 'Loss (Trending)', 'Lyrics', 'Fastest Rising', 'Beauty', 'When is...?', 'News Events', 'Global Football Players', 'Global Public Figures', 'Famous men', 'Songs', 'Top Ten', \"Top 'John' Searches\", 'News (Trending)', 'Overall', 'News Searches', 'People of Sport', 'Top trending searches', 'Premier League Football', 'Sports Personalities', 'Kiwis', 'Australian Celebrities', 'Top Olympians', 'What Is...', 'Aussie Public Figures', 'Global People', 'Top Movies', 'Who...?', 'Travel Destinations', 'Coronavirus', 'Cities', 'Music Bands', 'Popular Sports-Related Queries', 'Sporting Events', 'How to...', 'Sports Teams', 'UK Number 1 Singles', 'News Items of 2015', '\"How To\" Searches', 'When is', 'Men', 'Television', 'Delivery']\n",
      "outputs/wiki_categories_50k_r50_c5.csv generated.\n"
     ]
    }
   ],
   "source": [
    "create_all_output_files(original_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
